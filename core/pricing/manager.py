"""
–°–µ—Ä–≤–∏—Å —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ LLM –∏ –∑–∞—Ç—Ä–∞—Ç—ã.

–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è LiteLLM –∏ –∫—ç—à–∏—Ä—É–µ—Ç –∏—Ö –Ω–∞ 1 –¥–µ–Ω—å.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∏ –≤—ã–∑–æ–≤–µ LLM.
"""

import logging
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import anyio
import httpx
from dotenv import load_dotenv

from core.ai_models.models import BaseChatModel
from core.ai_models.models import ChatInvokeUsage
from core.pricing.models import (
	CachedPricingData,
	ModelPricing,
	ModelUsageStats,
	ModelUsageTokens,
	TokenCostCalculated,
	TokenUsageEntry,
	UsageSummary,
)
from core.helpers import create_task_with_error_handling

load_dotenv()

from core.config import CONFIG

logger = logging.getLogger(__name__)
cost_logger = logging.getLogger('cost')

# –ú–∞–ø–ø–∏–Ω–≥ –æ—Ç –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∫ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ LiteLLM
MODEL_TO_LITELLM: dict[str, str] = {
	'gemini-flash-latest': 'gemini/gemini-flash-latest',
}

# –ö–∞—Å—Ç–æ–º–Ω–æ–µ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π, –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤ –¥–∞–Ω–Ω—ã—Ö LiteLLM.
# –¶–µ–Ω—ã —É–∫–∞–∑–∞–Ω—ã –∑–∞ —Ç–æ–∫–µ–Ω (–Ω–µ –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤).
# –§–æ—Ä–º–∞—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–µ model_prices_and_context_window.json –æ—Ç LiteLLM
CUSTOM_MODEL_PRICING: dict[str, dict[str, Any]] = {
	'bu-1-0': {
		'output_cost_per_token': 2.00 / 1_000_000,  # $3.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
		'input_cost_per_token': 0.2 / 1_000_000,  # $0.50 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
		'cache_read_input_token_cost': 0.02 / 1_000_000,  # $0.10 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
		'cache_creation_input_token_cost': None,  # –ù–µ —É–∫–∞–∑–∞–Ω–æ
		'max_output_tokens': None,  # –ù–µ —É–∫–∞–∑–∞–Ω–æ
		'max_input_tokens': None,  # –ù–µ —É–∫–∞–∑–∞–Ω–æ
		'max_tokens': None,  # –ù–µ —É–∫–∞–∑–∞–Ω–æ
	}
}

CUSTOM_MODEL_PRICING['smart'] = CUSTOM_MODEL_PRICING['bu-1-0']
CUSTOM_MODEL_PRICING['bu-latest'] = CUSTOM_MODEL_PRICING['bu-1-0']


def xdg_cache_home() -> Path:
	default_path = Path.home() / '.cache'
	if CONFIG.XDG_CACHE_HOME and (cache_path := Path(CONFIG.XDG_CACHE_HOME)).is_absolute():
		return cache_path
	return default_path


class TokenCost:
	"""–°–µ—Ä–≤–∏—Å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Ä–∞—Å—á–µ—Ç–∞ –∑–∞—Ç—Ä–∞—Ç"""

	CACHE_DIR_NAME = 'agent/token_cost'
	CACHE_DURATION = timedelta(days=1)
	PRICING_URL = 'https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json'

	def __init__(self, include_cost: bool = False):
		env_calculate_cost = os.getenv('AGENT_CALCULATE_COST', 'false').lower() == 'true'
		self.include_cost = include_cost or env_calculate_cost

		self.usage_history: list[TokenUsageEntry] = []
		self.registered_llms: dict[str, BaseChatModel] = {}
		self._pricing_data: dict[str, Any] | None = None
		self._initialized = False
		self._cache_dir = xdg_cache_home() / self.CACHE_DIR_NAME

	async def initialize(self) -> None:
		"""–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ä–≤–∏—Å –ø—É—Ç–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ —Ü–µ–Ω–∞—Ö"""
		if not self._initialized:
			if self.include_cost:
				await self._load_pricing_data()
			self._initialized = True

	async def _load_pricing_data(self) -> None:
		"""–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –ø–æ–ª—É—á–∏—Ç—å —Å GitHub"""
		# –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –∫—ç—à–∞
		valid_cache_file = await self._find_valid_cache()

		if valid_cache_file:
			await self._load_from_cache(valid_cache_file)
		else:
			await self._fetch_and_cache_pricing_data()

	async def _find_valid_cache(self) -> Path | None:
		"""–ù–∞–π—Ç–∏ —Å–∞–º—ã–π –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –∫—ç—à–∞"""
		try:
			# –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
			self._cache_dir.mkdir(parents=True, exist_ok=True)

			# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö JSON —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫—ç—à–∞
			json_cache_files = list(self._cache_dir.glob('*.json'))

			if not json_cache_files:
				return None

			# –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å–∞–º—ã–π –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–µ—Ä–≤—ã–π)
			json_cache_files.sort(key=lambda file: file.stat().st_mtime, reverse=True)

			# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–º –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π
			for cache_file_path in json_cache_files:
				if await self._is_cache_valid(cache_file_path):
					return cache_file_path
				else:
					# –û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã –∫—ç—à–∞
					try:
						os.remove(cache_file_path)
					except Exception:
						pass

			return None
		except Exception:
			return None

	async def _is_cache_valid(self, cache_file_path: Path) -> bool:
		"""–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª –∫—ç—à–∞ –∏ –Ω–µ –∏—Å—Ç–µ–∫ –ª–∏ —Å—Ä–æ–∫ –µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è"""
		try:
			if not cache_file_path.exists():
				return False

			# –ü—Ä–æ—á–∏—Ç–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
			cached_data = CachedPricingData.model_validate_json(await anyio.Path(cache_file_path).read_text())

			# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –ª–∏ –µ—â–µ –∫—ç—à
			time_difference = datetime.now() - cached_data.timestamp
			return time_difference < self.CACHE_DURATION
		except Exception:
			return False

	async def _load_from_cache(self, cache_file_path: Path) -> None:
		"""–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∫—ç—à–∞"""
		try:
			file_content = await anyio.Path(cache_file_path).read_text()
			cached_data = CachedPricingData.model_validate_json(file_content)
			self._pricing_data = cached_data.data
		except Exception as load_error:
			logger.debug(f'Error loading cached pricing data from {cache_file_path}: {load_error}')
			# –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –ø–æ–ª—É—á–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö
			await self._fetch_and_cache_pricing_data()

	async def _fetch_and_cache_pricing_data(self) -> None:
		"""–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö –∏–∑ LiteLLM GitHub –∏ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –∏—Ö —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π"""
		try:
			async with httpx.AsyncClient() as http_client:
				http_response = await http_client.get(self.PRICING_URL, timeout=30)
				http_response.raise_for_status()

				self._pricing_data = http_response.json()

			# –°–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç –∫—ç—à–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
			now = datetime.now()
			cached_data = CachedPricingData(timestamp=now, data=self._pricing_data or {})

			# –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
			self._cache_dir.mkdir(parents=True, exist_ok=True)

			# –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª –∫—ç—à–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
			timestamp_string = now.strftime('%Y%m%d_%H%M%S')
			cache_file_path = self._cache_dir / f'pricing_{timestamp_string}.json'

			await anyio.Path(cache_file_path).write_text(cached_data.model_dump_json(indent=2))
		except Exception as fetch_error:
			logger.debug(f'Error fetching pricing data: {fetch_error}')
			# –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –ø—É—Å—Ç—ã–º –¥–∞–Ω–Ω—ã–º –æ —Ü–µ–Ω–∞—Ö
			self._pricing_data = {}

	async def get_model_pricing(self, model_name: str) -> ModelPricing | None:
		"""–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–Ω–∞—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
		# –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
		if not self._initialized:
			await self.initialize()

		# –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ü–µ–Ω—ã
		if model_name in CUSTOM_MODEL_PRICING:
			custom_data = CUSTOM_MODEL_PRICING[model_name]
			return ModelPricing(
				model=model_name,
				output_cost_per_token=custom_data.get('output_cost_per_token'),
				input_cost_per_token=custom_data.get('input_cost_per_token'),
				max_output_tokens=custom_data.get('max_output_tokens'),
				max_input_tokens=custom_data.get('max_input_tokens'),
				max_tokens=custom_data.get('max_tokens'),
				cache_creation_input_token_cost=custom_data.get('cache_creation_input_token_cost'),
				cache_read_input_token_cost=custom_data.get('cache_read_input_token_cost'),
			)

		# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏ –≤ –∏–º—è –º–æ–¥–µ–ª–∏ LiteLLM, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
		mapped_model_name = MODEL_TO_LITELLM.get(model_name, model_name)

		if not self._pricing_data or mapped_model_name not in self._pricing_data:
			return None

		pricing_data = self._pricing_data[mapped_model_name]
		return ModelPricing(
			model=model_name,
			output_cost_per_token=pricing_data.get('output_cost_per_token'),
			input_cost_per_token=pricing_data.get('input_cost_per_token'),
			max_output_tokens=pricing_data.get('max_output_tokens'),
			max_input_tokens=pricing_data.get('max_input_tokens'),
			max_tokens=pricing_data.get('max_tokens'),
			cache_creation_input_token_cost=pricing_data.get('cache_creation_input_token_cost'),
			cache_read_input_token_cost=pricing_data.get('cache_read_input_token_cost'),
		)

	async def calculate_cost(self, model: str, usage: ChatInvokeUsage) -> TokenCostCalculated | None:
		if not self.include_cost:
			return None

		pricing_info = await self.get_model_pricing(model)
		if pricing_info is None:
			return None

		cached_tokens_count = usage.prompt_cached_tokens or 0
		uncached_prompt_tokens = usage.prompt_tokens - cached_tokens_count

		# –¢–æ–∫–µ–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
		completion_tokens_count = usage.completion_tokens
		completion_cost_value = completion_tokens_count * float(pricing_info.output_cost_per_token or 0)

		# –ù–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –ø—Ä–æ–º–ø—Ç–∞
		new_prompt_cost_value = uncached_prompt_tokens * (pricing_info.input_cost_per_token or 0)

		# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
		read_cached_cost = None
		if cached_tokens_count and pricing_info.cache_read_input_token_cost:
			read_cached_cost = cached_tokens_count * pricing_info.cache_read_input_token_cost

		# –¢–æ–∫–µ–Ω—ã —Å–æ–∑–¥–∞–Ω–∏—è –∫—ç—à–∞
		creation_tokens_count = usage.prompt_cache_creation_tokens
		creation_cost = None
		if pricing_info.cache_creation_input_token_cost and creation_tokens_count:
			creation_cost = creation_tokens_count * pricing_info.cache_creation_input_token_cost

		return TokenCostCalculated(
			completion_tokens=completion_tokens_count,
			completion_cost=completion_cost_value,
			new_prompt_tokens=usage.prompt_tokens,
			new_prompt_cost=new_prompt_cost_value,
			prompt_read_cached_tokens=usage.prompt_cached_tokens,
			prompt_read_cached_cost=read_cached_cost,
			prompt_cached_creation_tokens=creation_tokens_count,
			prompt_cache_creation_cost=creation_cost,
		)

	def add_usage(self, model: str, usage: ChatInvokeUsage) -> TokenUsageEntry:
		"""–î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏—é (–±–µ–∑ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏)"""
		usage_entry = TokenUsageEntry(
			timestamp=datetime.now(),
			model=model,
			usage=usage,
		)

		self.usage_history.append(usage_entry)

		return usage_entry


	async def _log_usage(self, model: str, usage_entry: TokenUsageEntry) -> None:
		"""–ó–∞–ø–∏—Å–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –ª–æ–≥–≥–µ—Ä"""
		if not self._initialized:
			await self.initialize()

		# ANSI –∫–æ–¥—ã —Ü–≤–µ—Ç–æ–≤
		CYAN_COLOR = '\033[96m'
		YELLOW_COLOR = '\033[93m'
		GREEN_COLOR = '\033[92m'
		BLUE_COLOR = '\033[94m'
		RESET_COLOR = '\033[0m'

		# –í—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–±–∏–≤–∫—É —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π —Ç–æ–∫–µ–Ω–æ–≤ (–¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞—Ç—Ä–∞—Ç—ã)
		cost_data = await self.calculate_cost(model, usage_entry.usage)

		# –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ä–∞–∑–±–∏–≤–∫—É –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
		input_display = self._build_input_tokens_display(usage_entry.usage, cost_data)

		# –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
		completion_tokens_formatted = self._format_tokens(usage_entry.usage.completion_tokens)
		if self.include_cost and cost_data and cost_data.completion_cost > 0:
			output_display = f'üì§ {GREEN_COLOR}{completion_tokens_formatted} (${cost_data.completion_cost:.4f}){RESET_COLOR}'
		else:
			output_display = f'üì§ {GREEN_COLOR}{completion_tokens_formatted}{RESET_COLOR}'

		cost_logger.debug(f'üß† {CYAN_COLOR}{model}{RESET_COLOR} | {input_display} | {output_display}')

	def _build_input_tokens_display(self, usage: ChatInvokeUsage, cost_data: TokenCostCalculated | None) -> str:
		"""–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —á–µ—Ç–∫–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–±–∏–≤–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å —ç–º–æ–¥–∑–∏ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏"""
		YELLOW_COLOR = '\033[93m'
		BLUE_COLOR = '\033[94m'
		RESET_COLOR = '\033[0m'

		display_parts = []

		# –í—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ä–∞–∑–±–∏–≤–∫—É —Ç–æ–∫–µ–Ω–æ–≤, –µ—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç
		if usage.prompt_cached_tokens or usage.prompt_cache_creation_tokens:
			# –í—ã—á–∏—Å–ª–∏—Ç—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–Ω–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
			cached_count = usage.prompt_cached_tokens or 0
			new_tokens_count = usage.prompt_tokens - cached_count

			if new_tokens_count > 0:
				new_tokens_formatted = self._format_tokens(new_tokens_count)
				if self.include_cost and cost_data and cost_data.new_prompt_cost > 0:
					display_parts.append(f'üÜï {YELLOW_COLOR}{new_tokens_formatted} (${cost_data.new_prompt_cost:.4f}){RESET_COLOR}')
				else:
					display_parts.append(f'üÜï {YELLOW_COLOR}{new_tokens_formatted}{RESET_COLOR}')

			if usage.prompt_cached_tokens:
				cached_tokens_formatted = self._format_tokens(usage.prompt_cached_tokens)
				if self.include_cost and cost_data and cost_data.prompt_read_cached_cost:
					display_parts.append(f'üíæ {BLUE_COLOR}{cached_tokens_formatted} (${cost_data.prompt_read_cached_cost:.4f}){RESET_COLOR}')
				else:
					display_parts.append(f'üíæ {BLUE_COLOR}{cached_tokens_formatted}{RESET_COLOR}')

			if usage.prompt_cache_creation_tokens:
				creation_tokens_formatted = self._format_tokens(usage.prompt_cache_creation_tokens)
				if self.include_cost and cost_data and cost_data.prompt_cache_creation_cost:
					display_parts.append(f'üîß {BLUE_COLOR}{creation_tokens_formatted} (${cost_data.prompt_cache_creation_cost:.4f}){RESET_COLOR}')
				else:
					display_parts.append(f'üîß {BLUE_COLOR}{creation_tokens_formatted}{RESET_COLOR}')

		if not display_parts:
			# –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
			total_tokens_formatted = self._format_tokens(usage.prompt_tokens)
			if self.include_cost and cost_data and cost_data.new_prompt_cost > 0:
				display_parts.append(f'üì• {YELLOW_COLOR}{total_tokens_formatted} (${cost_data.new_prompt_cost:.4f}){RESET_COLOR}')
			else:
				display_parts.append(f'üì• {YELLOW_COLOR}{total_tokens_formatted}{RESET_COLOR}')

		return ' + '.join(display_parts)

	def register_llm(self, llm: BaseChatModel) -> BaseChatModel:
		"""
		–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å LLM –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤

		@dev –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —ç–∫–∑–µ–º–ø–ª—è—Ä –Ω–µ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
		"""
		# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ID —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–ª—é—á–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–ª–ª–∏–∑–∏–π –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞–º–∏
		llm_instance_id = str(id(llm))

		# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ —ç—Ç–æ—Ç —Ç–æ—á–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
		if llm_instance_id in self.registered_llms:
			logger.debug(f'LLM instance {llm_instance_id} ({llm.provider}_{llm.model}) is already registered')
			return llm

		self.registered_llms[llm_instance_id] = llm

		# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –º–µ—Ç–æ–¥
		original_ainvoke_method = llm.ainvoke
		# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ self –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∑–∞–º—ã–∫–∞–Ω–∏–∏
		service_instance = self

		# –°–æ–∑–¥–∞—Ç—å –æ–±–µ—Ä–Ω—É—Ç—É—é –≤–µ—Ä—Å–∏—é, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
		async def tracked_ainvoke(messages, output_format=None, **kwargs):
			# –í—ã–∑–≤–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –º–µ—Ç–æ–¥, –ø–µ—Ä–µ–¥–∞–≤–∞—è –ª—é–±—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ kwargs
			invoke_result = await original_ainvoke_method(messages, output_format, **kwargs)

			# –û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ (await –Ω–µ –Ω—É–∂–µ–Ω, —Ç–∞–∫ –∫–∞–∫ add_usage —Ç–µ–ø–µ—Ä—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π)
			# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å llm.model –≤–º–µ—Å—Ç–æ llm.name –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å get_usage_tokens_for_model()
			if invoke_result.usage:
				usage_entry = service_instance.add_usage(llm.model, invoke_result.usage)

				logger.debug(f'Token cost service: {usage_entry}')

				create_task_with_error_handling(
					service_instance._log_usage(llm.model, usage_entry), name='log_token_usage', suppress_exceptions=True
				)

			# else:
			# 	await service_instance._log_non_usage_llm(llm)

			return invoke_result

		# –ó–∞–º–µ–Ω–∏—Ç—å –º–µ—Ç–æ–¥ –Ω–∞—à–µ–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º–æ–π –≤–µ—Ä—Å–∏–µ–π
		# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ setattr –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–∏–ø–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
		setattr(llm, 'ainvoke', tracked_ainvoke)

		return llm

	def get_usage_tokens_for_model(self, model: str) -> ModelUsageTokens:
		"""–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
		model_usage_entries = [entry for entry in self.usage_history if entry.model == model]

		return ModelUsageTokens(
			model=model,
			completion_tokens=sum(entry.usage.completion_tokens for entry in model_usage_entries),
			prompt_cached_tokens=sum(entry.usage.prompt_cached_tokens or 0 for entry in model_usage_entries),
			prompt_tokens=sum(entry.usage.prompt_tokens for entry in model_usage_entries),
			total_tokens=sum(entry.usage.prompt_tokens + entry.usage.completion_tokens for entry in model_usage_entries),
		)

	async def get_usage_summary(self, model: str | None = None, since: datetime | None = None) -> UsageSummary:
		"""–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∑–∞—Ç—Ä–∞—Ç (–∑–∞—Ç—Ä–∞—Ç—ã –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –Ω–∞ –ª–µ—Ç—É)"""
		filtered_entries = self.usage_history

		if model:
			filtered_entries = [entry for entry in filtered_entries if entry.model == model]

		if since:
			filtered_entries = [entry for entry in filtered_entries if entry.timestamp >= since]

		if not filtered_entries:
			return UsageSummary(
				total_completion_tokens=0,
				total_completion_cost=0.0,
				total_tokens=0,
				total_cost=0.0,
				total_prompt_tokens=0,
				total_prompt_cost=0.0,
				total_prompt_cached_tokens=0,
				total_prompt_cached_cost=0.0,
				entry_count=0,
			)

		# –í—ã—á–∏—Å–ª–∏—Ç—å –∏—Ç–æ–≥–∏
		total_completion = sum(entry.usage.completion_tokens for entry in filtered_entries)
		total_prompt = sum(entry.usage.prompt_tokens for entry in filtered_entries)
		total_tokens_count = total_prompt + total_completion
		total_prompt_cached = sum(entry.usage.prompt_cached_tokens or 0 for entry in filtered_entries)
		unique_models = list({entry.model for entry in filtered_entries})

		# –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–æ–¥–µ–ª—è–º —Å —Ä–∞—Å—á–µ—Ç–æ–º –∑–∞—Ç—Ä–∞—Ç –∑–∞–ø–∏—Å—å –∑–∞ –∑–∞–ø–∏—Å—å—é
		per_model_stats: dict[str, ModelUsageStats] = {}
		total_completion_cost = 0.0
		total_prompt_cost = 0.0
		total_prompt_cached_cost = 0.0

		for usage_entry in filtered_entries:
			if usage_entry.model not in per_model_stats:
				per_model_stats[usage_entry.model] = ModelUsageStats(model=usage_entry.model)

			model_statistics = per_model_stats[usage_entry.model]
			model_statistics.completion_tokens += usage_entry.usage.completion_tokens
			model_statistics.prompt_tokens += usage_entry.usage.prompt_tokens
			model_statistics.total_tokens += usage_entry.usage.prompt_tokens + usage_entry.usage.completion_tokens
			model_statistics.invocations += 1

			if self.include_cost:
				# –í—ã—á–∏—Å–ª–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –∑–∞–ø–∏—Å—å –∑–∞ –∑–∞–ø–∏—Å—å—é –∏—Å–ø–æ–ª—å–∑—É—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é calculate_cost
				cost_calculation = await self.calculate_cost(usage_entry.model, usage_entry.usage)
				if cost_calculation:
					model_statistics.cost += cost_calculation.total_cost
					total_completion_cost += cost_calculation.completion_cost
					total_prompt_cost += cost_calculation.prompt_cost
					total_prompt_cached_cost += cost_calculation.prompt_read_cached_cost or 0

		# –í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
		for model_statistics in per_model_stats.values():
			if model_statistics.invocations > 0:
				model_statistics.average_tokens_per_invocation = model_statistics.total_tokens / model_statistics.invocations

		return UsageSummary(
			total_completion_tokens=total_completion,
			total_completion_cost=total_completion_cost,
			total_tokens=total_tokens_count,
			total_cost=total_completion_cost + total_prompt_cost + total_prompt_cached_cost,
			total_prompt_tokens=total_prompt,
			total_prompt_cost=total_prompt_cost,
			total_prompt_cached_tokens=total_prompt_cached,
			total_prompt_cached_cost=total_prompt_cached_cost,
			entry_count=len(filtered_entries),
			by_model=per_model_stats,
		)

	def _format_tokens(self, token_count: int) -> str:
		"""–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º k –¥–ª—è —Ç—ã—Å—è—á"""
		if token_count >= 1000000000:
			return f'{token_count / 1000000000:.1f}B'
		if token_count >= 1000000:
			return f'{token_count / 1000000:.1f}M'
		if token_count >= 1000:
			return f'{token_count / 1000:.1f}k'
		return str(token_count)

	async def log_usage_summary(self) -> None:
		"""–ó–∞–ø–∏—Å–∞—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–≤–æ–¥–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ –º–æ–¥–µ–ª—è–º —Å —Ü–≤–µ—Ç–∞–º–∏ –∏ –∫—Ä–∞—Å–∏–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
		if not self.usage_history:
			return

		usage_summary = await self.get_usage_summary()

		if usage_summary.entry_count == 0:
			return

		# ANSI –∫–æ–¥—ã —Ü–≤–µ—Ç–æ–≤
		CYAN_COLOR = '\033[96m'
		YELLOW_COLOR = '\033[93m'
		GREEN_COLOR = '\033[92m'
		BLUE_COLOR = '\033[94m'
		MAGENTA_COLOR = '\033[95m'
		RESET_COLOR = '\033[0m'
		BOLD_COLOR = '\033[1m'

		# –ó–∞–ø–∏—Å–∞—Ç—å –æ–±—â—É—é —Å–≤–æ–¥–∫—É
		total_tokens_formatted = self._format_tokens(usage_summary.total_tokens)
		completion_tokens_formatted = self._format_tokens(usage_summary.total_completion_tokens)
		prompt_tokens_formatted = self._format_tokens(usage_summary.total_prompt_tokens)

		# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–±–∏–≤–∫—É –∑–∞—Ç—Ä–∞—Ç –¥–ª—è –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∑–∞—Ç—Ä–∞—Ç –≤–∫–ª—é—á–µ–Ω–æ)
		if self.include_cost and usage_summary.total_cost > 0:
			total_cost_display = f' (${MAGENTA_COLOR}{usage_summary.total_cost:.4f}{RESET_COLOR})'
			completion_cost_display = f' (${usage_summary.total_completion_cost:.4f})'
			prompt_cost_display = f' (${usage_summary.total_prompt_cost:.4f})'
		else:
			total_cost_display = ''
			completion_cost_display = ''
			prompt_cost_display = ''

		if len(usage_summary.by_model) > 1:
			cost_logger.debug(
				f'üí≤ {BOLD_COLOR}Total Usage Summary{RESET_COLOR}: {BLUE_COLOR}{total_tokens_formatted} tokens{RESET_COLOR}{total_cost_display} | '
				f'‚¨ÖÔ∏è {YELLOW_COLOR}{prompt_tokens_formatted}{prompt_cost_display}{RESET_COLOR} | ‚û°Ô∏è {GREEN_COLOR}{completion_tokens_formatted}{completion_cost_display}{RESET_COLOR}'
			)

		for model_name, model_statistics in usage_summary.by_model.items():
			# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã
			model_total_formatted = self._format_tokens(model_statistics.total_tokens)
			model_completion_formatted = self._format_tokens(model_statistics.completion_tokens)
			model_prompt_formatted = self._format_tokens(model_statistics.prompt_tokens)
			avg_tokens_formatted = self._format_tokens(int(model_statistics.average_tokens_per_invocation))

			# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞—Ç—Ä–∞—Ç (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∑–∞—Ç—Ä–∞—Ç –≤–∫–ª—é—á–µ–Ω–æ)
			if self.include_cost:
				# –í—ã—á–∏—Å–ª–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º –Ω–∞ –ª–µ—Ç—É
				model_completion_cost = 0.0
				model_prompt_cost = 0.0

				# –í—ã—á–∏—Å–ª–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
				for history_entry in self.usage_history:
					if history_entry.model == model_name:
						entry_cost = await self.calculate_cost(history_entry.model, history_entry.usage)
						if entry_cost:
							model_completion_cost += entry_cost.completion_cost
							model_prompt_cost += entry_cost.prompt_cost

				total_model_cost = model_completion_cost + model_prompt_cost

				if total_model_cost > 0:
					cost_display = f' (${MAGENTA_COLOR}{total_model_cost:.4f}{RESET_COLOR})'
					completion_display = f'{GREEN_COLOR}{model_completion_formatted} (${model_completion_cost:.4f}){RESET_COLOR}'
					prompt_display = f'{YELLOW_COLOR}{model_prompt_formatted} (${model_prompt_cost:.4f}){RESET_COLOR}'
				else:
					cost_display = ''
					completion_display = f'{GREEN_COLOR}{model_completion_formatted}{RESET_COLOR}'
					prompt_display = f'{YELLOW_COLOR}{model_prompt_formatted}{RESET_COLOR}'
			else:
				cost_display = ''
				completion_display = f'{GREEN_COLOR}{model_completion_formatted}{RESET_COLOR}'
				prompt_display = f'{YELLOW_COLOR}{model_prompt_formatted}{RESET_COLOR}'

			cost_logger.debug(
				f'  ü§ñ {CYAN_COLOR}{model_name}{RESET_COLOR}: {BLUE_COLOR}{model_total_formatted} tokens{RESET_COLOR}{cost_display} | '
				f'‚¨ÖÔ∏è {prompt_display} | ‚û°Ô∏è {completion_display} | '
				f'üìû {model_statistics.invocations} calls | üìà {avg_tokens_formatted}/call'
			)

	async def get_cost_by_model(self) -> dict[str, ModelUsageStats]:
		"""–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–±–∏–≤–∫—É –∑–∞—Ç—Ä–∞—Ç –ø–æ –º–æ–¥–µ–ª—è–º"""
		usage_summary = await self.get_usage_summary()
		return usage_summary.by_model

	def clear_history(self) -> None:
		"""–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
		self.usage_history = []

	async def refresh_pricing_data(self) -> None:
		"""–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö —Å GitHub"""
		if self.include_cost:
			await self._fetch_and_cache_pricing_data()

	async def clean_old_caches(self, keep_count: int = 3) -> None:
		"""–û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã –∫—ç—à–∞, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ"""
		try:
			# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö JSON —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫—ç—à–∞
			all_cache_files = list(self._cache_dir.glob('*.json'))

			if len(all_cache_files) <= keep_count:
				return

			# –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –ø–µ—Ä–≤—ã–µ)
			all_cache_files.sort(key=lambda file: file.stat().st_mtime)

			# –£–¥–∞–ª–∏—Ç—å –≤—Å–µ, –∫—Ä–æ–º–µ —Å–∞–º—ã—Ö –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤
			for old_cache_file in all_cache_files[:-keep_count]:
				try:
					os.remove(old_cache_file)
				except Exception:
					pass
		except Exception as cleanup_error:
			logger.debug(f'Error cleaning old cache files: {cleanup_error}')

	async def ensure_pricing_loaded(self) -> None:
		"""–£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ. –í—ã–∑–≤–∞—Ç—å —ç—Ç–æ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞."""
		if not self._initialized and self.include_cost:
			# –≠—Ç–æ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –∏ –Ω–µ –±—É–¥–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å
			await self.initialize()
